{"cells":[{"cell_type":"code","source":["import os\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\nfrom pyspark.sql.functions import from_unixtime, to_date, col, lower, regexp_replace, dense_rank, when\nfrom pyspark.conf import SparkConf\nfrom pyspark.sql.window import Window\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f4d29346-7d53-440c-9692-8f82b3817898","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Cargamos los datos de metadata que el grupo de analysts unio y realizo el etl correspondiente para analisis\ndf = spark.read.format(\"parquet\").load(\"/mnt/backupaws/metadata/metada_newyork/part-00000-8b5573bf-55b1-4249-9e02-80a376685fc7-c000.snappy.parquet\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1b202f0-b460-4ee8-aa92-60eb5c07f5c9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Primero vamos a crear un ID unico por cada sitio que tengamos en la metadata para esto primero realizamos un drop por si se repite la misma ubicacion de un local"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ee8e5d29-53df-44c5-9066-0482f43ca790","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# en caso tengamos filas duplicadas en la metadata realizamos un drop con el gmap_id\ndf = df.dropDuplicates(['gmap_id'])\n\n# Creamos una ventana por la columna name, ordenada por name\nwindow2 = Window.orderBy(\"gmap_id\")\n\n# Agregamos una nueva columna id_name a la tabla, usando dense_rank() para asignar un número de identificación único a cada valor de name\ndf = df.withColumn(\"id_name_empresa\", dense_rank().over(window2))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8308ad61-2876-454b-996b-c84c08a54bd8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Realizamos el filtrado por lugar, en este caso empezaremos por RESTAURANT y seleccionamos solo las columnas que vayamos a usar para el entrenamiento del modelo"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6f1065e0-5bd3-4bf8-89f6-44038783df51","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# seleccionamos solo los datos a usar en el modelo\ndate_sitios_ml = df.select('id_name_empresa','name', 'gmap_id', 'category')\n\n# filtramos solo el rubro de restaurantes que es en lo que se enfocara el sistema de recomendacion\ndate_sitios_ml = date_sitios_ml.filter(lower(df[\"category\"]).like(\"%bar %\"))\n\n# Eliminamos la columna category\ndate_delivery = date_sitios_ml.drop('category')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b8999fcc-f35f-4894-a1f0-0baada55c8f1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Cargamos ahora los reviews para poder hacer el ETL correspondiente para ML\ndf_1= spark.read.format(\"parquet\").load(\"/mnt/backupaws/reviews/18/part-00000-f610cba4-3bf4-481a-a9b6-e7313f0c3e80-c000.snappy.parquet\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"332887a7-45d6-4369-b53f-76ae4c3e5d4e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Primero creamos una columna igual a la columna user_id pero lo convertimos a tipo de dato string\ndf_1 = df_1.withColumn(\"col_id\", col(\"user_id\").cast(\"string\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ecdf7e6b-b9b1-43b1-ad93-542ceae0818d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Ordenamos la columna col_id \nwindow = Window.orderBy(\"col_id\")\n\n# Agregamos una nueva columna id_name a la tabla, usando dense_rank() para asignar un número de identificación único a cada valor de name\ndata = df_1.withColumn(\"id_name\", dense_rank().over(window))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"79cb2dcb-9f29-4943-b1d2-5d995b316196","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# hacemos el respectivo join para poder tener los datos listos para hacer el entrenamiento, el objetivo es tener solo 3 columnas(id_name, id_name_empresa, rating) que son las columnas a usar para el modelo als de recomendacion"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"03a14713-7e78-47b3-a8fa-1052e7733550","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# filtramos los datos que vamos a usar del dataframe de nueva york\ndate_newyork_ml = data.select('id_name', 'rating', 'gmap_id')\n\n# filtramos los datos que necesitamos para el join\ndate_sitios_ml = date_sitios_ml.select('id_name_empresa', 'gmap_id')\n\n# hacemos un join de los datos que tenemos tanto de sitios como de nueva york usando como id en comun el gmap_id\ndf_join = date_sitios_ml.join(date_newyork_ml, \"gmap_id\").join(date_delivery, 'id_name_empresa')\n\n# hacemos el drop de gmap_id que no lo necesitamos\ndf_test = df_join.drop('gmap_id')\n\n# realizamos el segundo drop\ndf_test = df_test.drop('name')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ede92305-4258-4c0f-919d-9da9faa37481","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Una vez hecho estos procesos anteriores empezamos con la separacion del entrenamiento del modelo y del testeo con un 80% de datos de entrenamiento"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a3160ce5-6e7b-420c-a2ed-c16ee48a77bc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# train test validation split\n\nseed = 1234\n(training_df, test_df) = df_test.randomSplit([.8, .2], seed=seed)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0a0f3bd1-4ba9-4296-82c9-94b62fa5d92b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Ejecutamos el codigo para asegurarnos que al momento de guardar los datos en formato parquet nos guarde en un solo archivo .parquet\ntraining_df = training_df.coalesce(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"34609ebe-901d-422b-aa37-d4c26d253469","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ejecutamos el guardado de los datos para su proceso de ML con ALS y nos de un poco de rapidez al momento de hacer el cross validator\ntraining_df.write.mode(\"overwrite\").parquet(\"/mnt/backupaws/Datos_ML_BAR/Datos_Bar_entrenamiento\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"431f5e5e-3ff5-49ca-b37a-b5883a235b49","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Ejecutamos el codigo para asegurarnos que al momento de guardar los datos en formato parquet nos guarde en un solo archivo .parquet\ntest_df = test_df.coalesce(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"18b958f0-fd21-44af-974f-b5c280ced464","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ejecutamos el guardado de los datos para su proceso de ML con ALS y nos de un poco de rapidez al momento de hacer el cross validator\ntest_df.write.mode(\"overwrite\").parquet(\"/mnt/backupaws/Datos_ML_BAR/Datos_Bar_testeo\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fd382a15-0ef0-4e9c-99b0-a83e26e34492","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ETL-BAR","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":492347552461281}},"nbformat":4,"nbformat_minor":0}
